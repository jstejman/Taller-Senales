{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taller de Procesamiento de Señales - TP3 Regresión Logística\n",
    "### Alumno: Julián Stejman\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "1) Calcular la función inversa $\\sigma^{-1}(p)$ con $p \\in (0,1)$\n",
    "2) Sea $p = \\sigma(z)$ la función sigmoide, calcular la derivada $\\sigma'(z)$ en términos de p.\n",
    "3) Hallar una expresión analítica para la función costo y su gradiente. Tenga en cuenta el modelo asociado a una regresión logística de dos clases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $$p = \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "$$\n",
    "\\frac{1}{p} = 1+e^{-z}\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{p} -1= \\frac{1-p}{p}=e^{-z}\n",
    "$$\n",
    "$$\n",
    "log(\\frac{1-p}{p}) = -z\n",
    "$$\n",
    "$$\n",
    "\\sigma^{-1}(p) = log(\\frac{p}{1-p})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  $$p'= \\sigma'(z) = \\left[ \\frac{1}{1+e^{-z}}\\right]' = \\frac{e^{-z}}{(1+e^{-z})^2}$$\n",
    "$$\n",
    "= \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1+ e^{-z} - 1}{(1+e^{-z})^2} = \\frac{1+ e^{-z}}{(1+e^{-z})^2} - \\frac{1}{(1+e^{-z})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1+e^{-z}} - \\frac{1}{(1+e^{-z})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1+e^{-z}}\\left[1- \\frac{1}{1+e^{-z}} \\right] = p(1-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. La función de costo que se utilizará es la Binary Cross-Entropy que tiene la siguiente forma:\n",
    "$$\n",
    "J(w,b) = -\\frac{1}{N}\\sum_{i=1}^N Y_i log(\\sigma(\\hat{Y_i})) + (1-Y_i)log(1-\\sigma(\\hat{Y_i}))\n",
    "$$\n",
    "$Y_i$ representa la etiqueta real de $X_i$ y la función $\\sigma(\\hat{Y_i})$ es análoga a la estimación de la probabilidad de $Y_i$ si uno considera que $\\hat{Y}_i = w^TX_i + b$\n",
    "\n",
    "Si se desea hallar $\\nabla J(w,b)$ es equivalente a hacer:\n",
    "$$\n",
    "\\nabla J(w,b) = -\\frac{1}{n}\\sum_{i=1}^N Y_i \\nabla \\left(log(\\sigma(\\hat{Y_i}))\\right) + (1-Y_i)\\nabla \\left(log(1-\\sigma(\\hat{Y_i}))\\right)\n",
    "$$\n",
    "\n",
    "Se busca entonces hallar esos gradientes:\n",
    "\n",
    " - $\\nabla log(\\sigma(\\hat{Y_i})) = (1-\\sigma(\\hat{Y_i}))\\left[X_i\\;\\; 1\\right]^T$\n",
    "-  $\\nabla log(1-\\sigma(\\hat{Y_i})) = -\\sigma(\\hat{Y_i})\\left[X_i\\;\\; 1\\right]^T$\n",
    "\n",
    "Entonces se tiene finalmente que el gradiente tiene esta forma:\n",
    "\n",
    "$$\n",
    "\\nabla J(w,b) = \\frac{1}{N}\\sum_{i=0}^N \\left(\\sigma(\\hat{Y_i}) - Y_i\\right)\\left[X_i\\;\\; 1\\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
